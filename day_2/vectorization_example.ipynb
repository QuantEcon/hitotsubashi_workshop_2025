{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad70c1d",
   "metadata": {},
   "source": [
    "# Inventory Management Model: Vectorization Practice\n",
    "\n",
    "*Prepared for the Computational Economics Workshop at Hitotsubashi*\n",
    "\n",
    "Author: [John Stachurski](https://johnstachurski.net)\n",
    "\n",
    "This notebook demonstrates a stochastic dynamic inventory management model and compares different computational approaches for calculating transition probabilities.\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "We have an inventory system with:\n",
    "\n",
    "- $K$: Maximum inventory capacity\n",
    "- $p$: Parameter for demand shock distribution\n",
    "\n",
    "Inventory evolves according to \n",
    "\n",
    "$$\n",
    "    X_{t+1} = \\max(X_t - D_{t+1}, 0) + A_t\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $X_t$ is current inventory (number of units),\n",
    "- $D_{t+1}$ is an IID demand shock, and\n",
    "- $A_t$ is the current order (number of units).\n",
    "\n",
    "We are interested in computing the transition probability kernel\n",
    "\n",
    "$$P(x, a, y) := \\mathbb P\\{X_{t+1}=y \\,|\\, X_t = x, A_t = a \\}$$\n",
    "\n",
    "More explicitly,\n",
    "\n",
    "$$P(x, a, y) = \\sum_{d \\geq 0} \\mathbb{1}\\{\\max(x - d, 0) + a = y\\} \\phi(d)$$\n",
    "\n",
    "Here\n",
    "\n",
    "- $d$ is the demand shock\n",
    "- $\\phi$ is the probability density function for demand\n",
    "\n",
    "\n",
    "## Mathematical Derivation\n",
    "\n",
    "The transition probability kernel obeys\n",
    "\n",
    "\\begin{align}\n",
    "P(x, a, y) &= \\sum_{d \\geq 0} \\mathbb{1}\\{\\max(x - d, 0) + a = y\\} \\phi(d) \\\\\n",
    "&= \\sum_{d < x} \\mathbb{1}\\{x - d + a = y\\} \\phi(d) + \\sum_{d \\geq x} \\mathbb{1}\\{a = y\\} \\phi(d) \\\\\n",
    "&= \\sum_{d < x} \\mathbb{1}\\{d = x + a - y\\} \\phi(d) + \\mathbb{1}\\{y = a\\} F(x) \\\\\n",
    "&= \\mathbb{1}\\{0 \\leq x + a - y < x\\} \\phi(x + a - y) + \\mathbb{1}\\{y = a\\} F(x)\n",
    "\\end{align}\n",
    "\n",
    "Where $F(x) = P\\{D \\geq x\\}$ is the survival function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7698d4f4",
   "metadata": {},
   "source": [
    "## Implementation Approaches\n",
    "\n",
    "We'll compare three different computational approaches:\n",
    "1. **Loop-based** (Numba JIT compiled)\n",
    "2. **Vectorized** (JAX vectorized operations)\n",
    "3. **Vmap** (JAX's functional transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ae2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import NamedTuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0263185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NamedTuple):\n",
    "    K: int = 50     # max inventory\n",
    "    p: float = 0.6  # demand shock parameter\n",
    "\n",
    "\n",
    "def ϕ(p, d):\n",
    "    \"\"\"PDF for demand shock: ϕ(d) = (1-p)^d * p\"\"\"\n",
    "    return (1 - p)**d * p\n",
    "\n",
    "\n",
    "def F(p, x):\n",
    "    \"\"\"Survival function: F(x) = P{D ≥ x} = (1-p)^x\"\"\"\n",
    "    return (1 - p)**x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a9f388",
   "metadata": {},
   "source": [
    "Let's create a model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "print(f\"Created {model}\")\n",
    "print(f\"Demand PDF: ϕ(d) = (1-p)^d * p = (1-{model.p})^d * {model.p}\")\n",
    "print(f\"Survival function: F(x) = (1-p)^x = (1-{model.p})^x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f995fe",
   "metadata": {},
   "source": [
    "## Method 1: Loop-based Implementation (Numba)\n",
    "\n",
    "This approach uses traditional nested loops with Numba JIT compilation for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3103141",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def generate_kernel_loops(model):\n",
    "    \"\"\"\n",
    "    Loop-based computation of the transition probability kernel P(x, a, y).\n",
    "    See the mathematical derivation above for the complete formula.\n",
    "    \"\"\"\n",
    "    K, p = model\n",
    "    S = K + 1\n",
    "    P = np.zeros((S, S, S))\n",
    "\n",
    "    def ϕ(d):\n",
    "        return (1 - p)**d * p\n",
    "\n",
    "    def F(x):\n",
    "        return (1 - p)**x\n",
    "\n",
    "    for x in range(S):\n",
    "        for a in range(S):\n",
    "            for y in range(S):\n",
    "                # implement 1{0 ≤ x + a - y < x} φ(x + a - y) + 1{y = a} F(x)\n",
    "                phi_value = (0 <= x + a - y < x) * ϕ(x + a - y) \n",
    "                f_value = (y == a) * F(x)\n",
    "                P[x, a, y] = phi_value + f_value\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23caad",
   "metadata": {},
   "source": [
    "## Method 2: Vectorized Implementation (JAX)\n",
    "\n",
    "This approach uses JAX's vectorized operations to compute all probabilities simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe9a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kernel_vectorized(model):\n",
    "    \"\"\"\n",
    "    Fully vectorized JAX-based computation of the transition probability kernel P(x, a, y).\n",
    "    See the mathematical derivation above for the complete formula.\n",
    "    \"\"\"\n",
    "    K, p = model\n",
    "    S = K + 1\n",
    "    \n",
    "    # Create meshgrids for vectorized computation\n",
    "    x_grid, a_grid, y_grid = jnp.meshgrid(\n",
    "        jnp.arange(S), jnp.arange(S), jnp.arange(S), indexing='ij'\n",
    "    )\n",
    "    \n",
    "    # Initialize probability tensor\n",
    "    P = jnp.zeros((S, S, S))\n",
    "    \n",
    "    # Vectorized computation of the first term: 1{0 ≤ x + a - y < x} φ(x + a - y)\n",
    "    d_candidate = x_grid + a_grid - y_grid\n",
    "    valid_d = jnp.logical_and(d_candidate >= 0, d_candidate < x_grid) \n",
    "    phi_values = valid_d * ϕ(p, d_candidate)\n",
    "    \n",
    "    # Second term: I{y = a} F(x)\n",
    "    y_eq_a = jnp.equal(y_grid, a_grid)\n",
    "    f_values = y_eq_a * F(p, x_grid) \n",
    "  \n",
    "    # Combine both terms\n",
    "    P = phi_values + f_values\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d114a",
   "metadata": {},
   "source": [
    "## Method 3: Vmap Implementation (JAX)\n",
    "\n",
    "This approach uses JAX's `vmap` (vectorized map) to transform a scalar function into a vectorized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kernel_vmap(model): \n",
    "    \"\"\"\n",
    "    Vmap-based computation of the transition probability kernel P(x, a, y).\n",
    "    Uses JAX's vmap to vectorize the scalar function over all (x, a, y) combinations.\n",
    "    \"\"\"\n",
    "    K, p = model\n",
    "    S = K + 1\n",
    "\n",
    "    def P(x, a, y):\n",
    "        \"\"\"\n",
    "        Scalar function to compute P(x, a, y) for a single (x, a, y) triple.\n",
    "        See the mathematical derivation above for the complete formula.\n",
    "        \"\"\"\n",
    "        d = x + a - y\n",
    "        # Test 0 <= x + a - y < x (first term)\n",
    "        valid_d = jnp.logical_and(0 <= d, d < x)\n",
    "        # Test y = a (second term)\n",
    "        y_eq_a = jnp.equal(y, a)\n",
    "        # Combine: 1{0 ≤ x + a - y < x} φ(x + a - y) + 1{y = a} F(x)\n",
    "        return valid_d * ϕ(p, d) + y_eq_a * F(p, x)\n",
    "\n",
    "    # Create all combinations of (x, a, y) indices\n",
    "    x_vals = jnp.arange(S)\n",
    "    a_vals = jnp.arange(S)  \n",
    "    y_vals = jnp.arange(S)\n",
    "    \n",
    "    # Use vmap to compute P(x,a,y) for all combinations\n",
    "    vmap_y = jax.vmap(P,      (None, None, 0))\n",
    "    vmap_a = jax.vmap(vmap_y, (None, 0, None))\n",
    "    vmap_x = jax.vmap(vmap_a, (0, None, None))\n",
    "    \n",
    "    return vmap_x(x_vals, a_vals, y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa7458a",
   "metadata": {},
   "source": [
    "## Correctness Verification\n",
    "\n",
    "Let's verify that all three methods produce identical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934576c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute results using all three methods\n",
    "loops_P = generate_kernel_loops(model)\n",
    "vectorized_P = generate_kernel_vectorized(model)\n",
    "vmap_P = generate_kernel_vmap(model)\n",
    "\n",
    "print(\"=== Correctness Check ===\")\n",
    "print(f\"Vectorized P equals loops P: {np.allclose(loops_P, vectorized_P)}\")\n",
    "print(f\"Vmap P equals loops P: {np.allclose(loops_P, vmap_P)}\")\n",
    "print()\n",
    "\n",
    "# Show some sample probabilities\n",
    "print(\"Sample transition probabilities P(x=2, a=1, y):\")\n",
    "for y in range(min(6, model.K + 1)):\n",
    "    prob = loops_P[2, 1, y]\n",
    "    print(f\"P(2, 1, {y}) = {prob:.6f}\")\n",
    "print()\n",
    "\n",
    "# Verify that probabilities sum to 1 for each (x, a) pair\n",
    "prob_sums = np.sum(loops_P, axis=2)\n",
    "print(f\"All probability sums equal 1: {np.allclose(prob_sums, 1.0)}\")\n",
    "print(f\"Max deviation from 1: {np.max(np.abs(prob_sums - 1.0)):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848fd4d",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "Now let's compare the performance of all three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7df94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JIT-compiled versions\n",
    "gen_kernel_vectorized_jit = jax.jit(generate_kernel_vectorized, static_argnums=(0,))\n",
    "gen_kernel_vmapped_jit = jax.jit(generate_kernel_vmap, static_argnums=(0,))\n",
    "\n",
    "# Warm up JIT compilation\n",
    "print(\"Warming up JIT compilation...\")\n",
    "_ = generate_kernel_loops(model)\n",
    "_ = gen_kernel_vectorized_jit(model)\n",
    "_ = gen_kernel_vmapped_jit(model)\n",
    "\n",
    "print(\"Warmup complete.\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed74a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run performance benchmarks\n",
    "print(\"Running performance benchmarks (100 iterations each)...\")\n",
    "print()\n",
    "\n",
    "times_loops_jit = []\n",
    "times_vec_jit = []\n",
    "times_vmap_jit = []\n",
    "\n",
    "n_iterations = 1000\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Progress: {i + 1}/{n_iterations}\")\n",
    "    \n",
    "    # Benchmark loops\n",
    "    start = time.perf_counter()\n",
    "    _ = generate_kernel_loops(model)\n",
    "    end = time.perf_counter()\n",
    "    times_loops_jit.append(end - start)\n",
    "\n",
    "    # Benchmark vectorized\n",
    "    start = time.perf_counter()\n",
    "    _ = gen_kernel_vectorized_jit(model).block_until_ready()\n",
    "    end = time.perf_counter()\n",
    "    times_vec_jit.append(end - start)\n",
    "    \n",
    "    # Benchmark vmap\n",
    "    start = time.perf_counter()\n",
    "    _ = gen_kernel_vmapped_jit(model).block_until_ready()\n",
    "    end = time.perf_counter()\n",
    "    times_vmap_jit.append(end - start)\n",
    "\n",
    "print(\"\\n=== Performance Results ===\")\n",
    "print(f\"Loops (Numba):  {np.mean(times_loops_jit)*1000000:.1f} ± {np.std(times_loops_jit)*1000000:.1f} μs\")\n",
    "print(f\"Vectorized:     {np.mean(times_vec_jit)*1000000:.1f} ± {np.std(times_vec_jit)*1000000:.1f} μs\")\n",
    "print(f\"Vmap:           {np.mean(times_vmap_jit)*1000000:.1f} ± {np.std(times_vmap_jit)*1000000:.1f} μs\")\n",
    "print()\n",
    "\n",
    "# Calculate speedups\n",
    "loops_mean = np.mean(times_loops_jit)\n",
    "vec_mean = np.mean(times_vec_jit) \n",
    "vmap_mean = np.mean(times_vmap_jit)\n",
    "\n",
    "print(\"=== Relative Performance ===\")\n",
    "print(f\"Vectorized vs Loops: {loops_mean / vec_mean:.1f}x {'faster' if vec_mean < loops_mean else 'slower'}\")\n",
    "print(f\"Vmap vs Loops:       {loops_mean / vmap_mean:.1f}x {'faster' if vmap_mean < loops_mean else 'slower'}\")\n",
    "print(f\"Vmap vs Vectorized:  {vec_mean / vmap_mean:.1f}x {'faster' if vmap_mean < vec_mean else 'slower'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05586b29",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated three different approaches to computing transition probabilities in a stochastic inventory model:\n",
    "\n",
    "1. **Loop-based with Numba**: Traditional nested loops with JIT compilation\n",
    "2. **Vectorized JAX**: Fully vectorized operations using meshgrids\n",
    "3. **Vmap JAX**: Functional transformation of scalar operations\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "- All three methods produce identical results, confirming correctness\n",
    "- The JAX-based approaches are faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad1ddd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
