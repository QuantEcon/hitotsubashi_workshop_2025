{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a812fe1e",
   "metadata": {},
   "source": [
    "# Job Search\n",
    "\n",
    "*Prepared for the Computational Economics Workshop at Hitotsubashi*\n",
    "\n",
    "Author: [John Stachurski](https://johnstachurski.net)\n",
    "\n",
    "\n",
    "In this lecture we study a basic infinite-horizon job search problem with Markov wage\n",
    "draws \n",
    "\n",
    "* For background on infinite horizon job search see, e.g., [DP1](https://dp.quantecon.org/).\n",
    "\n",
    "\n",
    "In addition to what's in Anaconda, this lecture will need the QE library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install quantecon  # Uncomment if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09255a5a",
   "metadata": {},
   "source": [
    "We use the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import quantecon as qe\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import lax\n",
    "import time\n",
    "from typing import NamedTuple, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458eae6",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We study an elementary model where \n",
    "\n",
    "* jobs are permanent \n",
    "* unemployed workers receive current compensation $c$\n",
    "* the horizon is infinite\n",
    "* an unemployment agent discounts the future via discount factor $\\beta \\in (0,1)$\n",
    "\n",
    "### Set up\n",
    "\n",
    "At the start of each period, an unemployed worker receives wage offer $W_t$.\n",
    "\n",
    "We assume that \n",
    "\n",
    "$$\n",
    "    W_{t+1} = \\rho W_t + \\nu Z_{t+1}\n",
    "$$\n",
    "\n",
    "where $(Z_t)_{t \\geq 0}$ is IID and standard normal.\n",
    "\n",
    "We then discretize this wage process using Tauchen's method to produce a stochastic matrix $P$.\n",
    "\n",
    "Successive wage offers are drawn from $P$.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "Since jobs are permanent, the return to accepting wage offer $w$ today is\n",
    "\n",
    "$$\n",
    "    w + \\beta w + \\beta^2 w + \n",
    "    \\cdots = \\frac{w}{1-\\beta}\n",
    "$$\n",
    "\n",
    "The Bellman equation is\n",
    "\n",
    "$$\n",
    "    v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "We solve this model using value function iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e6b34",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "Let's set up a `Model` class to store information needed to solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5583e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NamedTuple):\n",
    "    n: int\n",
    "    w_vals: jnp.ndarray\n",
    "    P: jnp.ndarray\n",
    "    β: float\n",
    "    c: float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07702ebc",
   "metadata": {},
   "source": [
    "The function below holds default values and creates a `Model` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af3c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_js_model(\n",
    "        n: int = 500,       # wage grid size\n",
    "        ρ: float = 0.9,     # wage persistence\n",
    "        ν: float = 0.2,     # wage volatility\n",
    "        β: float = 0.99,    # discount factor\n",
    "        c: float = 1.0,     # unemployment compensation\n",
    "    ) -> Model:\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), jnp.array(mc.P)\n",
    "    return Model(n, w_vals, P, β, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc050b09",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421cbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_js_model(β=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10570280",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85186d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b6fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.w_vals.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec83c3",
   "metadata": {},
   "source": [
    "Here's the Bellman operator\n",
    "\n",
    "$$\n",
    "    (Tv)(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "    \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a907527",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T(v: jnp.ndarray, model: Model) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β P v} with\n",
    "\n",
    "        e(w) = w / (1-β) and (Pv)(w) = E_w[ v(W')]\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c = model\n",
    "    h = c + β * P @ v\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c9f078",
   "metadata": {},
   "source": [
    "The next function computes the optimal policy under the assumption that $v$ is\n",
    "the value function.\n",
    "\n",
    "The policy takes the form\n",
    "\n",
    "$$\n",
    "    \\sigma(w) = \\mathbf 1 \n",
    "        \\left\\{\n",
    "            \\frac{w}{1-\\beta} \\geq c + \\beta \\sum_{w'} v(w') P(w, w')\n",
    "        \\right\\}\n",
    "$$\n",
    "\n",
    "Here $\\mathbf 1$ is an indicator function.\n",
    "\n",
    "* $\\sigma(w) = 1$ means stop\n",
    "* $\\sigma(w) = 0$ means continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88354389",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_greedy(v: jnp.ndarray, model: Model) -> jnp.ndarray:\n",
    "    \"Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + β * P @ v\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90c515",
   "metadata": {},
   "source": [
    "Here's a routine for value function iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def vfi_step(v: jnp.ndarray, model: Model) -> tuple[jnp.ndarray, float]:\n",
    "    \"\"\"Single step of value function iteration.\"\"\"\n",
    "    new_v = T(v, model)\n",
    "    error = jnp.max(jnp.abs(new_v - v))\n",
    "    return new_v, error\n",
    "\n",
    "def vfi(\n",
    "        model: Model,\n",
    "        max_iter: int = 10_000,\n",
    "        tol: float = 1e-4,\n",
    "        verbose: bool = False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Solve the infinite-horizon Markov job search model by VFI.\n",
    "\n",
    "    \"\"\"\n",
    "    v = jnp.zeros_like(model.w_vals)  # Initial condition\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        new_v, error = vfi_step(v, model)\n",
    "\n",
    "        if error < tol:\n",
    "            if verbose:\n",
    "                print(f\"VFI converged after {i+1} iterations (error: {error:.2e})\")\n",
    "            break\n",
    "        v = new_v\n",
    "    else:\n",
    "        print(f\"VFI hit max iterations ({max_iter}) with error {error:.2e}\")\n",
    "\n",
    "    return new_v, get_greedy(new_v, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabc46f6",
   "metadata": {},
   "source": [
    "## Computing the solution\n",
    "\n",
    "Let's set up and solve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_js_model()\n",
    "n, w_vals, P, β, c = model\n",
    "\n",
    "v_star, σ_star = vfi(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff997b",
   "metadata": {},
   "source": [
    "Here's the optimal policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(σ_star)\n",
    "ax.set_xlabel(\"wage values\")\n",
    "ax.set_ylabel(\"optimal choice (stop=1)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cadc2",
   "metadata": {},
   "source": [
    "Let's compute the runtime as well, averaging over a number of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtimes = []\n",
    "for _ in range(10):\n",
    "    start = time.time()\n",
    "    v_star, σ_star = vfi(model, verbose=False)\n",
    "    end = time.time()\n",
    "    runtimes.append(end - start)\n",
    "\n",
    "print()\n",
    "print(f\"Mean runtime for value function iteration = {jnp.mean(jnp.array(runtimes)):.4f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329137ac",
   "metadata": {},
   "source": [
    "We compute the reservation wage as the first $w$ such that $\\sigma(w)=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2eb948",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_indices = jnp.where(σ_star == 1)\n",
    "stop_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wage_index = min(stop_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e1d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_wage = w_vals[res_wage_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e8dda",
   "metadata": {},
   "source": [
    "Here's a joint plot of the value function and the reservation wage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star, alpha=0.8, label=\"value function\")\n",
    "ax.vlines((res_wage,), 150, 400, 'k', ls='--', label=\"reservation wage\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1899271",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In the setting above, the agent is risk-neutral vis-a-vis future utility risk.\n",
    "\n",
    "Now solve the same problem but this time assuming that the agent has risk-sensitive\n",
    "preferences, which are a type of nonlinear recursive preferences.\n",
    "\n",
    "The Bellman equation becomes\n",
    "\n",
    "$$\n",
    "    v(w) = \\max\n",
    "    \\left\\{\n",
    "            \\frac{w}{1-\\beta}, \n",
    "            c + \\frac{\\beta}{\\theta}\n",
    "            \\ln \\left[ \n",
    "                      \\sum_{w'} \\exp(\\theta v(w')) P(w, w')\n",
    "                \\right]\n",
    "    \\right\\}\n",
    "$$\n",
    "\n",
    "\n",
    "When $\\theta < 0$ the agent is risk averse.\n",
    "\n",
    "Solve the model when $\\theta = -0.1$ and compare your result to the risk neutral\n",
    "case.\n",
    "\n",
    "Try to interpret your result.\n",
    "\n",
    "You can start with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e90114",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskModel(NamedTuple):\n",
    "    n: int\n",
    "    w_vals: jnp.ndarray\n",
    "    P: jnp.ndarray\n",
    "    β: float\n",
    "    c: float\n",
    "    θ: float\n",
    "\n",
    "def create_risk_sensitive_js_model(\n",
    "        n: int = 500,       # wage grid size\n",
    "        ρ: float = 0.9,     # wage persistence\n",
    "        ν: float = 0.2,     # wage volatility\n",
    "        β: float = 0.99,    # discount factor\n",
    "        c: float = 1.0,     # unemployment compensation\n",
    "        θ: float = -0.1     # risk parameter\n",
    "    ) -> RiskModel:\n",
    "    \"Creates an instance of the job search model with Markov wages.\"\n",
    "    mc = qe.tauchen(n, ρ, ν)\n",
    "    w_vals, P = jnp.exp(mc.state_values), jnp.array(mc.P)\n",
    "    return RiskModel(n, w_vals, P, β, c, θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203fb8a",
   "metadata": {},
   "source": [
    "Now you need to modify `T` and `get_greedy` and then run value function iteration again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cbed48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(15):\n",
    "    print(\"Solution below!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T_rs(v: jnp.ndarray, model: RiskModel) -> jnp.ndarray:\n",
    "    \"\"\"\n",
    "    The Bellman operator Tv = max{e, c + β R v} with\n",
    "\n",
    "        e(w) = w / (1-β) and\n",
    "\n",
    "        (Rv)(w) = (1/θ) ln{E_w[ exp(θ v(W'))]}\n",
    "\n",
    "    \"\"\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    e = w_vals / (1 - β)\n",
    "\n",
    "    return jnp.maximum(e, h)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_greedy_rs(v: jnp.ndarray, model: RiskModel) -> jnp.ndarray:\n",
    "    \" Get a v-greedy policy.\"\n",
    "    n, w_vals, P, β, c, θ = model\n",
    "    e = w_vals / (1 - β)\n",
    "    h = c + (β / θ) * jnp.log(P @ (jnp.exp(θ * v)))\n",
    "    σ = jnp.where(e >= h, 1, 0)\n",
    "    return σ\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def vfi_rs_step(v: jnp.ndarray, model: RiskModel) -> tuple[jnp.ndarray, float]:\n",
    "    \"\"\"Single step of risk-sensitive value function iteration.\"\"\"\n",
    "    new_v = T_rs(v, model)\n",
    "    error = jnp.max(jnp.abs(new_v - v))\n",
    "    return new_v, error\n",
    "\n",
    "def vfi_rs(\n",
    "        model: RiskModel,\n",
    "        max_iter: int = 10_000,\n",
    "        tol: float = 1e-4\n",
    "    ):\n",
    "    \"Solve the infinite-horizon Markov job search model by VFI.\"\n",
    "    v = jnp.zeros_like(model.w_vals)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        new_v, error = vfi_rs_step(v, model)\n",
    "\n",
    "        if error < tol:\n",
    "            print(f\"VFI converged after {i+1} iterations (error: {error:.2e})\")\n",
    "            break\n",
    "        v = new_v\n",
    "    else:\n",
    "        print(f\"VFI reached max iterations ({max_iter}) with error {error:.2e}\")\n",
    "\n",
    "    return new_v, get_greedy_rs(new_v, model)\n",
    "\n",
    "\n",
    "\n",
    "model_rs = create_risk_sensitive_js_model()\n",
    "n, w_vals, P, β, c, θ = model_rs\n",
    "v_star_rs, σ_star_rs = vfi_rs(model_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9181ea",
   "metadata": {},
   "source": [
    "Let's plot the results together with the original risk neutral case and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfc50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_indices = jnp.where(σ_star_rs == 1)\n",
    "res_wage_index = min(stop_indices[0])\n",
    "res_wage_rs = w_vals[res_wage_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(w_vals, v_star,  ls='-', color='blue',\n",
    "        alpha=0.8, label=\"risk neutral $v$\")\n",
    "ax.plot(w_vals, v_star_rs, ls='-', color='orange',\n",
    "        alpha=0.8, label=\"risk sensitive $v$\")\n",
    "ax.vlines((res_wage,), 100, 400,  ls='--', color='blue',\n",
    "          alpha=0.5, label=r\"risk neutral $\\bar w$\")\n",
    "ax.vlines((res_wage_rs,), 100, 400, ls='--', color='orange',\n",
    "          alpha=0.5, label=r\"risk sensitive $\\bar w$\")\n",
    "ax.legend(frameon=False, fontsize=12, loc=\"lower right\")\n",
    "ax.set_xlabel(\"$w$\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a98fa6",
   "metadata": {},
   "source": [
    "The figure shows that the reservation wage under risk sensitive preferences (RS $\\bar w$) shifts down.\n",
    "\n",
    "This makes sense -- the agent does not like risk and hence is more inclined to\n",
    "accept the current offer, even when it's lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30775e1d",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "In the code above, we wrote two versions of VFI, one for each model.\n",
    "\n",
    "This is poor style because we are repeating logic.  \n",
    "\n",
    "Write one version of VFI that can work with both and test that it does the\n",
    "same job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0112c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(15):\n",
    "    print(\"Solution below!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77743da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vfi_step(bellman_operator: Callable):\n",
    "    \"\"\"Create a JIT-compiled VFI step function for a given Bellman operator.\"\"\"\n",
    "    @jax.jit\n",
    "    def vfi_step_generic(v: jnp.ndarray) -> tuple[jnp.ndarray, float]:\n",
    "        new_v = bellman_operator(v)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        return new_v, error\n",
    "    return vfi_step_generic\n",
    "\n",
    "def generic_vfi(\n",
    "        bellman_operator: Callable,\n",
    "        get_greedy_function: Callable,\n",
    "        v_zero: jnp.ndarray,\n",
    "        max_iter: int = 10_000,\n",
    "        tol: float = 1e-4\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Solve the infinite-horizon Markov job search model by VFI.\n",
    "\n",
    "    \"\"\"\n",
    "    v = v_zero\n",
    "    vfi_step_fn = create_vfi_step(bellman_operator)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        new_v, error = vfi_step_fn(v)\n",
    "\n",
    "        if error < tol:\n",
    "            print(f\"VFI converged after {i+1} iterations (error: {error:.2e})\")\n",
    "            break\n",
    "        v = new_v\n",
    "    else:\n",
    "        print(f\"VFI reached max iterations ({max_iter}) with error {error:.2e}\")\n",
    "\n",
    "    return new_v, get_greedy_function(new_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48bd6b8",
   "metadata": {},
   "source": [
    "## Fully JIT-compiled VFI with lax.while_loop\n",
    "\n",
    "This version uses JAX's `lax.while_loop` for maximum performance with early termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vfi_lax(bellman_operator: Callable, get_greedy_fn: Callable):\n",
    "    \"\"\"Create a fully JIT-compiled VFI function for specific operators.\"\"\"\n",
    "\n",
    "    @jax.jit\n",
    "    def vfi_lax_impl(v_init: jnp.ndarray,\n",
    "                     max_iter: int = 10000,\n",
    "                     tol: float = 1e-4) -> tuple[jnp.ndarray, jnp.ndarray, int, bool]:\n",
    "        \"\"\"\n",
    "        Fully JIT-compiled VFI using lax.while_loop.\n",
    "\n",
    "        Returns:\n",
    "            - v_star: final value function\n",
    "            - policy: optimal policy\n",
    "            - iterations: number of iterations used\n",
    "            - converged: whether algorithm converged\n",
    "        \"\"\"\n",
    "\n",
    "        def cond_fn(carry):\n",
    "            v, error, iteration, converged = carry\n",
    "            # Continue while not converged and under max iterations\n",
    "            return (error >= tol) & (iteration < max_iter)\n",
    "\n",
    "        def body_fn(carry):\n",
    "            v, error, iteration, converged = carry\n",
    "            new_v = bellman_operator(v)\n",
    "            new_error = jnp.max(jnp.abs(new_v - v))\n",
    "            new_iteration = iteration + 1\n",
    "            new_converged = new_error < tol\n",
    "            return new_v, new_error, new_iteration, new_converged\n",
    "\n",
    "        # Initial state: (value function, error, iteration count, converged flag)\n",
    "        init_carry = (v_init, jnp.inf, 0, False)\n",
    "\n",
    "        # Run the while loop\n",
    "        final_v, final_error, iterations, converged = lax.while_loop(cond_fn, body_fn, init_carry)\n",
    "\n",
    "        # Compute policy\n",
    "        policy = get_greedy_fn(final_v)\n",
    "\n",
    "        return final_v, policy, iterations, converged\n",
    "\n",
    "    return vfi_lax_impl\n",
    "\n",
    "\n",
    "def vfi_lax_standard(model: Model, max_iter: int = 10000, tol: float = 1e-4, verbose: bool = False):\n",
    "    \"\"\"LAX VFI for standard job search model.\"\"\"\n",
    "    bellman_closure = lambda v: T(v, model)\n",
    "    greedy_closure = lambda v: get_greedy(v, model)\n",
    "    vfi_lax_fn = create_vfi_lax(bellman_closure, greedy_closure)\n",
    "\n",
    "    v_init = jnp.zeros_like(model.w_vals)\n",
    "    v_star, policy, iterations, converged = vfi_lax_fn(v_init, max_iter, tol)\n",
    "\n",
    "    if verbose:\n",
    "        if converged:\n",
    "            print(f\"LAX VFI converged after {iterations} iterations\")\n",
    "        else:\n",
    "            print(f\"LAX VFI hit max iterations ({max_iter})\")\n",
    "\n",
    "    return v_star, policy\n",
    "\n",
    "\n",
    "def vfi_lax_risk(model: RiskModel, max_iter: int = 10000, tol: float = 1e-4, verbose: bool = False):\n",
    "    \"\"\"LAX VFI for risk-sensitive job search model.\"\"\"\n",
    "    bellman_closure = lambda v: T_rs(v, model)\n",
    "    greedy_closure = lambda v: get_greedy_rs(v, model)\n",
    "    vfi_lax_fn = create_vfi_lax(bellman_closure, greedy_closure)\n",
    "\n",
    "    v_init = jnp.zeros_like(model.w_vals)\n",
    "    v_star, policy, iterations, converged = vfi_lax_fn(v_init, max_iter, tol)\n",
    "\n",
    "    if verbose:\n",
    "        if converged:\n",
    "            print(f\"LAX VFI converged after {iterations} iterations\")\n",
    "        else:\n",
    "            print(f\"LAX VFI hit max iterations ({max_iter})\")\n",
    "\n",
    "    return v_star, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f0acc5",
   "metadata": {},
   "source": [
    "Let's test this with the original model (comparing the output of `vfi` and `generic_vfi`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c412369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_js_model()\n",
    "n, w_vals, P, β, c = model\n",
    "v_star_0, σ_star_0 = vfi(model)\n",
    "bellman_operator = lambda v: T(v, model)\n",
    "get_greedy_function = lambda v: get_greedy(v, model)\n",
    "v_zero = jnp.zeros_like(w_vals)\n",
    "v_star_1, σ_star_1 = generic_vfi(\n",
    "    bellman_operator, get_greedy_function, v_zero\n",
    ")\n",
    "\n",
    "correct = jnp.allclose(v_star_0, v_star_1) and jnp.allclose(σ_star_0, σ_star_1)\n",
    "print(f\"Success = {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7d193",
   "metadata": {},
   "source": [
    "Let's also test this set up with the risk sensitive model (comparing the output of `vfi_rs` and `generic_vfi`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_risk_sensitive_js_model()\n",
    "n, w_vals, P, β, c, θ = model_rs\n",
    "v_star_0, σ_star_0 = vfi_rs(model)\n",
    "bellman_operator = lambda v: T_rs(v, model)\n",
    "get_greedy_function = lambda v: get_greedy_rs(v, model)\n",
    "v_zero = jnp.zeros_like(w_vals)\n",
    "v_star_1, σ_star_1 = generic_vfi(\n",
    "    bellman_operator, get_greedy_function, v_zero\n",
    ")\n",
    "\n",
    "correct = jnp.allclose(v_star_0, v_star_1) and jnp.allclose(σ_star_0, σ_star_1)\n",
    "print(f\"Success = {correct}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
