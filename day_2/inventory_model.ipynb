{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0d3a0c",
   "metadata": {},
   "source": [
    "# Inventory Management with JAX\n",
    "\n",
    "*Prepared for the Computational Economics Workshop at Hitotsubashi*\n",
    "\n",
    "Author: [John Stachurski](https://johnstachurski.net)\n",
    "\n",
    "This notebook studies a stochastic dynamic inventory management model and\n",
    "computes the optimal policy using\n",
    "value function iteration (VFI) and Howard's policy iteration (HPI).\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "The notebook structure is as follows:\n",
    "\n",
    "1. **Model Setup**: Define the inventory dynamics, rewards, and transition probabilities\n",
    "2. **Implement algorithms**: Implement VFI and HPI \n",
    "2. **Solve**: Compute an optimal policy\n",
    "5. **Simulation**: Monte Carlo simulation of inventory dynamics under the optimal policy\n",
    "\n",
    "The notebook uses **JAX** for high-performance numerical computing with\n",
    "automatic differentiation and just-in-time compilation.\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "We have an inventory system with:\n",
    "\n",
    "- $K$: Maximum inventory capacity\n",
    "- $\\beta$: Discount factor\n",
    "- $c$: Marginal cost (unit cost per order)\n",
    "- $\\kappa$: Fixed cost of ordering\n",
    "- $\\phi$: Demand shock distribution\n",
    "\n",
    "Inventory evolves according to \n",
    "\n",
    "$$\n",
    "    X_{t+1} = \\max(X_t - D_{t+1}, 0) + A_t\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $X_t$ is current inventory (number of units),\n",
    "- $D_{t+1}$ is an IID demand shock, and\n",
    "- $A_t$ is the current order (number of units).\n",
    "\n",
    "It will be convenient to work with the transition probability kernel\n",
    "\n",
    "$$P(x, a, y) := \\mathbb P\\{X_{t+1}=y \\,|\\, X_t = x, A_t = a \\}$$\n",
    "\n",
    "With $\\phi$ as the probability density function for demand,\n",
    "the transition probability kernel obeys\n",
    "\n",
    "\\begin{align}\n",
    "P(x, a, y) &= \\sum_{d \\geq 0} \\mathbb{1}\\{\\max(x - d, 0) + a = y\\} \\phi(d) \\\\\n",
    "&= \\sum_{d < x} \\mathbb{1}\\{x - d + a = y\\} \\phi(d) + \\sum_{d \\geq x} \\mathbb{1}\\{a = y\\} \\phi(d) \\\\\n",
    "&= \\sum_{d < x} \\mathbb{1}\\{d = x + a - y\\} \\phi(d) + \\mathbb{1}\\{y = a\\} F(x) \\\\\n",
    "&= \\mathbb{1}\\{0 \\leq x + a - y < x\\} \\phi(x + a - y) + \\mathbb{1}\\{y = a\\} F(x)\n",
    "\\end{align}\n",
    "\n",
    "Where $F(x) = P\\{D \\geq x\\}$ is the survival function.\n",
    "\n",
    "We begin with imports for numerical computing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c46f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple, Callable\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471f139f",
   "metadata": {},
   "source": [
    "We begin by setting a global constant $K$ for maximum inventory capacity\n",
    "\n",
    "K is made global to simplify JAX compilation: K determines array shapes (state space size = K+1), so it\n",
    "   must be known at compile time. \n",
    "\n",
    "Making K global avoids needing to to pass model instances as static in @jax.jit.\n",
    "\n",
    "This is problematic because Model contains non-hashable arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde224b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constant for maximum inventory capacity\n",
    "K = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5867c",
   "metadata": {},
   "source": [
    "Our default demand shock distribution will be geometric on 0, 1, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d557bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ϕ_geometric(d, p=0.4):\n",
    "    \"\"\"PDF for demand shock: ϕ(d) = (1-p)^d * p\"\"\"\n",
    "    return (1 - p)**d * p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6c2af",
   "metadata": {},
   "source": [
    "Now we set up the `Model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(NamedTuple):\n",
    "        β: float            # discount factor\n",
    "        c: float            # marginal cost\n",
    "        κ: float            # fixed cost\n",
    "        demand_pmf: jnp.ndarray  # precomputed demand PMF array\n",
    "\n",
    "\n",
    "def create_model(β=0.98, c=0.1, κ=0.8, ϕ=ϕ_geometric, max_demand=100):\n",
    "    \"\"\"\n",
    "    Create a Model instance with precomputed demand PMF.\n",
    "\n",
    "    Parameters:\n",
    "    - β: discount factor\n",
    "    - c: marginal cost per unit\n",
    "    - κ: fixed cost of ordering\n",
    "    - ϕ: demand shock probability mass function\n",
    "    - max_demand: maximum demand for numerical computation\n",
    "    \"\"\"\n",
    "    d_vals = jnp.arange(max_demand + 1)\n",
    "    demand_pmf = ϕ(d_vals)\n",
    "    return Model(β=β, c=c, κ=κ, demand_pmf=demand_pmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ddbd61",
   "metadata": {},
   "source": [
    "## Transition Probabilities\n",
    "\n",
    "The transition probability functions implement the mathematical formula derived\n",
    "above. \n",
    "\n",
    "These functions define how inventory evolves stochastically over time.\n",
    "\n",
    "We begin with a function that computes $P(x, a, y)$ for a single combination of current state\n",
    "$x$, action $a$, and next state $y$. It implements the two-term formula:\n",
    "\n",
    "$$P(x, a, y) = \\mathbb{1}\\{0 \\leq x + a - y < x\\} \\phi(x + a - y) + \\mathbb{1}\\{y = a\\} F(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129062f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def P_function(model, x, a, y):\n",
    "    \"\"\"\n",
    "    Scalar function to compute P(x, a, y) for a single (x, a, y) triple.\n",
    "\n",
    "    Implements the transition probability for inventory evolution:\n",
    "    X_{t+1} = min(max(X_t - D_{t+1}, 0) + A_t, K)\n",
    "\n",
    "    Returns 0 if action a would violate capacity constraint x + a > K.\n",
    "    \"\"\"\n",
    "    β, c, κ, demand_pmf = model\n",
    "    max_demand = len(demand_pmf) - 1\n",
    "\n",
    "    # If action violates capacity constraint, return 0\n",
    "    capacity_violated = (x + a > K)\n",
    "\n",
    "    # Compute the first term 1{0 <= x + a - y < x} ϕ(x + a - y) \n",
    "    d = x + a - y\n",
    "    indicator = ((d >= 0) & (d < x) & (d <= max_demand))\n",
    "    term1 = indicator * demand_pmf[d]\n",
    "\n",
    "    # Compute the second term 1{y = a} F(x) where F(x) = P{D >= x}\n",
    "    survival_mask = jnp.arange(max_demand + 1) >= x\n",
    "    Fx = jnp.sum(demand_pmf * survival_mask)\n",
    "    indicator = (y == a)\n",
    "    term2 = indicator * Fx\n",
    "\n",
    "    # Return 0 if capacity violated, otherwise return computed probability\n",
    "    return jnp.where(capacity_violated, 0.0, term1 + term2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb7af2",
   "metadata": {},
   "source": [
    "### P_array(): Vectorized Computation with vmap\n",
    "\n",
    "This function leverages JAX's `vmap` to compute the entire 3D transition tensor $P(x, a, y)$ efficiently. \n",
    "\n",
    "The nested `vmap` operations vectorize over:\n",
    "\n",
    "1. Next states $y$ (innermost)\n",
    "2. Actions $a$ (middle) \n",
    "3. Current states $x$ (outermost)\n",
    "\n",
    "This creates a tensor of shape `(S, S, S)` where `S = K + 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def P_array(model): \n",
    "    \"\"\"\n",
    "    Vmap-based computation of the transition probability kernel P(x, a, y).\n",
    "    Uses JAX's vmap to vectorize the scalar function over all (x, a, y)\n",
    "    combinations.\n",
    "    \"\"\"\n",
    "    β, c, κ, demand_pmf = model\n",
    "    S = K + 1\n",
    "\n",
    "    # Create all combinations of (x, a, y) indices\n",
    "    x_vals = jnp.arange(S)\n",
    "    a_vals = jnp.arange(S)  \n",
    "    y_vals = jnp.arange(S)\n",
    "    \n",
    "    # Use vmap to compute P(x,a,y) for all combinations\n",
    "    P_vmap_y   = jax.vmap(P_function, (None, None, None, 0))\n",
    "    P_vmap_ay  = jax.vmap(P_vmap_y,   (None, None, 0, None))\n",
    "    P_vmap_xay = jax.vmap(P_vmap_ay,  (None, 0, None, None))\n",
    "\n",
    "    return P_vmap_xay(model, x_vals, a_vals, y_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cb6712",
   "metadata": {},
   "source": [
    "## Reward Structure: Economics of Inventory Management\n",
    "\n",
    "The reward functions capture the economic trade-offs in inventory management:\n",
    "\n",
    "This function implements the expected reward:\n",
    "\n",
    "$$r(x, a) = \\sum_{d \\geq 0} \\min(x, d) \\phi(d) - ca - \\kappa \\mathbb{1}\\{a > 0\\}$$\n",
    "\n",
    "The components are:\n",
    "- Revenue: $\\sum_{d \\geq 0} \\min(x, d) \\phi(d)$ - expected sales (limited by inventory)\n",
    "- Ordering Cost: $ca$ - variable cost proportional to order size\n",
    "- Fixed Cost: $\\kappa \\mathbb{1}\\{a > 0\\}$ - fixed cost incurred when ordering any positive amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bd8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function(model, x, a):\n",
    "    \"\"\"\n",
    "    The flow (expected) reward function\n",
    "\n",
    "        r(x, a) = Σ_{d >= 0} min(x, d) ϕ(d) - ca - κ (a > 0)\n",
    "\n",
    "    This function works with any demand distribution by computing the sum\n",
    "    numerically.\n",
    "    \"\"\"\n",
    "    β, c, κ, demand_pmf = model\n",
    "    max_demand = len(demand_pmf) - 1\n",
    "\n",
    "    d_vals = jnp.arange(max_demand + 1)\n",
    "    sales_per_demand = jnp.minimum(x, d_vals)\n",
    "    expected_sales = jnp.sum(sales_per_demand * demand_pmf)\n",
    "    return expected_sales - c * a - jnp.greater(a, 0) * κ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1616a4e",
   "metadata": {},
   "source": [
    "Next we use `vmap` to compute the reward matrix $r(x, a)$ for all state-action\n",
    "pairs simultaneously. \n",
    "\n",
    "This 2D array has shape `(S, S)` and is used extensively in both VFI and policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3343d145",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def reward_array(model):\n",
    "    \"\"\"\n",
    "    Vmap-based computation of the reward array r(x, a).\n",
    "\n",
    "    \"\"\"\n",
    "    β, c, κ, demand_pmf = model\n",
    "    S = K + 1\n",
    "\n",
    "    # Create all combinations of (x, a) indices\n",
    "    x_vals = jnp.arange(S)\n",
    "    a_vals = jnp.arange(S)  \n",
    "    \n",
    "    # Use vmap to compute r(x,a) for all combinations\n",
    "    r_vmap_a   = jax.vmap(reward_function, (None, None, 0))\n",
    "    r_vmap_xa  = jax.vmap(r_vmap_a,        (None, 0,    None))\n",
    "    \n",
    "    return r_vmap_xa(model, x_vals, a_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01771198",
   "metadata": {},
   "source": [
    "## Value Function Iteration (VFI)\n",
    "\n",
    "For VFI we repeatedly apply the Bellman operator until convergence.\n",
    "\n",
    "First we implement the Bellman operator:\n",
    "\n",
    "$$T(v)(x) = \\max_a \\left[ r(x, a) + \\beta \\sum_y P(x, a, y) v(y) \\right]$$\n",
    "\n",
    "We use the following steps:\n",
    "\n",
    "1. Action-Value Computation: $B = r + \\beta \\sum_y P \\cdot v$ computes $Q(x,a)$ for all state-action pairs\n",
    "2. Policy Extraction: `jnp.max(B, axis=1)` finds the maximum over actions for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e1ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def T(model, P, r, v):\n",
    "    β, c, κ, demand_pmf = model\n",
    "    B = r + β * jnp.sum(P * v, axis=2)\n",
    "    return jnp.max(B, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfffcf6",
   "metadata": {},
   "source": [
    "### vfi(): Iterative Convergence\n",
    "\n",
    "The main VFI algorithm iterates the Bellman operator:\n",
    "\n",
    "1. Start with initial guess $v^0 = 0$\n",
    "2. Update: $v^{k+1} = T(v^k)$\n",
    "3. Continue until $\\|v^{k+1} - v^k\\| < \\text{tolerance}$\n",
    "\n",
    "Under our assumptions, VFI converges to the unique optimal value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467dcf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfi(model, max_iter=10_000, tol=1e-6):\n",
    "    β, c, κ, demand_pmf = model\n",
    "    P = P_array(model)\n",
    "    r = reward_array(model)\n",
    "    error = tol + 1\n",
    "    i = 0\n",
    "    v = jnp.zeros(K+1)\n",
    "\n",
    "    while i < max_iter and error > tol:\n",
    "        new_v = T(model, P, r, v)\n",
    "        error = jnp.max(jnp.abs(new_v - v))\n",
    "        v = new_v\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92b1a8",
   "metadata": {},
   "source": [
    "## Howard's Policy Iteration: An Alternative Approach\n",
    "\n",
    "Policy iteration takes a different approach from VFI by alternating between two\n",
    "steps: policy evaluation and policy improvement. \n",
    "\n",
    "This method often converges in fewer iterations than VFI.\n",
    "\n",
    "### policy_evaluation(): Solving the Linear System\n",
    "\n",
    "Given a policy $\\sigma$, this function computes the value function by solving:\n",
    "\n",
    "$$(I - \\beta P_\\sigma) v = r_\\sigma$$\n",
    "\n",
    "The steps are:\n",
    "\n",
    "- P_sigma extraction: `P[jnp.arange(S), σ, :]` - transition matrix under policy $\\sigma$\n",
    "- r_sigma construction: `r[jnp.arange(S), σ]` - rewards under policy $\\sigma$ \n",
    "- Linear system solution: Direct matrix inversion using `jnp.linalg.solve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162cbfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def policy_evaluation(model, P, r, σ):\n",
    "    \"\"\"\n",
    "    Policy evaluation: solve (I - β P_σ) v = r_σ for value function v\n",
    "    where P_σ and r_σ are transition matrix and rewards under policy σ\n",
    "\n",
    "    Solves the linear system directly using matrix inversion\n",
    "    \"\"\"\n",
    "    β, c, κ, demand_pmf = model\n",
    "    S = K + 1\n",
    "\n",
    "    # Extract transition probabilities and rewards for the given policy\n",
    "    P_sigma = P[jnp.arange(S), σ, :]  \n",
    "    r_sigma = r[jnp.arange(S), σ]     # This is where r_sigma is constructed!\n",
    "\n",
    "    # Solve (I - β P_σ) v = r_σ directly\n",
    "    A = jnp.eye(S) - β * P_sigma\n",
    "    v = jnp.linalg.solve(A, r_sigma)\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417ab35",
   "metadata": {},
   "source": [
    "Let's look carefully at how we build r_sigma.\n",
    "\n",
    "The reward array `r` has shape `(S, S)` where `S = K + 1` is the size of our state space. \n",
    "\n",
    "The element `r[x, a]` represents the expected reward for being in state `x` and taking action `a`.\n",
    "\n",
    "Given a policy $\\sigma$ (represented as an array where `σ[x]` gives the action to take in state `x`), we need to extract the rewards that would be obtained by following this policy. \n",
    "\n",
    "The line\n",
    "\n",
    "```python\n",
    "r_sigma = r[jnp.arange(S), σ]\n",
    "```\n",
    "\n",
    "uses fancy indexing to select specific elements from the 2D reward array.\n",
    "\n",
    "1. `jnp.arange(S)` creates `[0, 1, 2, ..., S-1]` - the row indices (states)\n",
    "2. `σ` contains the column indices (actions for each state)\n",
    "3. `r[jnp.arange(S), σ]` selects `r[i, σ[i]]` for each `i`\n",
    "\n",
    "Here's a small example to illustrate r_sigma construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616835b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 4x4 reward matrix for illustration\n",
    "S_example = 4\n",
    "r_example = jnp.array([\n",
    "    [1.0, 2.0, 3.0, 4.0],  # rewards for state 0\n",
    "    [5.0, 6.0, 7.0, 8.0],  # rewards for state 1  \n",
    "    [9.0, 10.0, 11.0, 12.0], # rewards for state 2\n",
    "    [13.0, 14.0, 15.0, 16.0] # rewards for state 3\n",
    "])\n",
    "\n",
    "# Example policy: σ[x] = action to take in state x\n",
    "policy_example = jnp.array([2, 0, 3, 1])  # state 0→action 2, state 1→action 0, etc.\n",
    "\n",
    "print(\"Reward matrix r:\")\n",
    "print(r_example)\n",
    "print()\n",
    "print(\"Policy σ (action for each state):\", policy_example)\n",
    "print()\n",
    "\n",
    "# Construct r_sigma using advanced indexing\n",
    "state_indices = jnp.arange(S_example)  # [0, 1, 2, 3]\n",
    "r_sigma_example = r_example[state_indices, policy_example]\n",
    "\n",
    "print(\"State indices:\", state_indices)\n",
    "print(\"Advanced indexing: r[state_indices, policy]\")\n",
    "print(\"This selects:\")\n",
    "for i in range(S_example):\n",
    "    print(f\"  r[{i}, {policy_example[i]}] = {r_example[i, policy_example[i]]}\")\n",
    "print()\n",
    "print(\"Resulting r_sigma:\", r_sigma_example)\n",
    "print()\n",
    "print(\"Interpretation:\")\n",
    "for i in range(S_example):\n",
    "    print(f\"  State {i}: policy chooses action {policy_example[i]}, reward = {r_sigma_example[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7ab30",
   "metadata": {},
   "source": [
    "### policy_improvement(): Greedy Policy Update\n",
    "\n",
    "The next function computes the greedy policy with respect to current value function:\n",
    "\n",
    "$$\\sigma'(x) = \\arg\\max_a \\left[ r(x,a) + \\beta \\sum_y P(x,a,y) v(y) \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e08439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def policy_improvement(model, P, r, v):\n",
    "    \"\"\"\n",
    "    Policy improvement: compute greedy policy with respect to value function v\n",
    "    Returns new policy σ'(x) = argmax_a [r(x,a) + β Σ_y P(x,a,y) v(y)]\n",
    "    \"\"\"\n",
    "    β, c, κ, demand_pmf = model\n",
    "    \n",
    "    # Compute Q(x,a) = r(x,a) + β Σ_y P(x,a,y) v(y)\n",
    "    Q = r + β * jnp.sum(P * v, axis=2)\n",
    "    \n",
    "    # Return greedy policy\n",
    "    return jnp.argmax(Q, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e102e",
   "metadata": {},
   "source": [
    "### howard_policy_iteration(): The Complete Algorithm\n",
    "\n",
    "Now we can implement HPI:\n",
    "\n",
    "1. Initialize: Start with arbitrary policy (e.g., $\\sigma^0(x) = 0$ for all $x$)\n",
    "2. Policy Evaluation: Solve for $v^\\sigma$ given current policy $\\sigma$\n",
    "3. Policy Improvement: Compute greedy policy $\\sigma'$ with respect to $v^\\sigma$\n",
    "4. Convergence Check: If $\\sigma' = \\sigma$, stop; otherwise set $\\sigma = \\sigma'$ and repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f420eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def howard_policy_iteration(model, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Howard's policy iteration algorithm.\n",
    "    Alternates between policy evaluation and policy improvement until\n",
    "    convergence.\n",
    "    \"\"\"\n",
    "    β, c, κ, demand_pmf = model\n",
    "    S = K + 1\n",
    "    P = P_array(model)\n",
    "    r = reward_array(model)\n",
    "    \n",
    "    # Initialize with zero policy (order nothing)\n",
    "    σ = jnp.zeros(S, dtype=int)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Policy evaluation\n",
    "        v = policy_evaluation(model, P, r, σ)\n",
    "\n",
    "        # Policy improvement\n",
    "        new_σ = policy_improvement(model, P, r, v)\n",
    "\n",
    "        # Check for convergence\n",
    "        if jnp.array_equal(σ, new_σ):\n",
    "            return v, new_σ\n",
    "\n",
    "        σ = new_σ\n",
    "\n",
    "    return v, σ\n",
    "\n",
    "\n",
    "def get_optimal_policy(model, v):\n",
    "    \"\"\"\n",
    "    Extract the optimal policy from a value function using policy improvement.\n",
    "    \"\"\"\n",
    "    P = P_array(model)\n",
    "    r = reward_array(model)\n",
    "    return policy_improvement(model, P, r, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e07f8",
   "metadata": {},
   "source": [
    "## Solving the Model: Comparing VFI and Policy Iteration\n",
    "\n",
    "Now we solve the inventory management problem using both algorithms and compare their results.\n",
    "\n",
    "This comparison demonstrates that both methods converge to the same optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "β, c, κ, demand_pmf = model\n",
    "\n",
    "P = P_array(model)\n",
    "r = reward_array(model)\n",
    "\n",
    "# Solve using value function iteration\n",
    "print(\"Solving with Value Function Iteration...\")\n",
    "v_vfi = vfi(model)\n",
    "policy_vfi = get_optimal_policy(model, v_vfi)\n",
    "\n",
    "# Solve using Howard's policy iteration\n",
    "print(\"Solving with Howard's Policy Iteration...\")\n",
    "v_hpi, policy_hpi = howard_policy_iteration(model)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison of methods:\")\n",
    "print(f\"VFI vs HPI - Value functions equal: \"\n",
    "      f\"{jnp.allclose(v_vfi, v_hpi, atol=1e-6)}\")\n",
    "print(f\"VFI vs HPI - Policies equal: \"\n",
    "      f\"{jnp.array_equal(policy_vfi, policy_hpi)}\")\n",
    "\n",
    "print(\"\\nOptimal Policy (inventory level -> order amount):\")\n",
    "for x in range(K + 1):\n",
    "    print(f\"  x={x}: order {policy_vfi[x]} units\")\n",
    "\n",
    "v = v_vfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6171c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot value function\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(range(K + 1), v, 'o-', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Inventory Level')\n",
    "ax.set_ylabel('Value Function')\n",
    "ax.set_title('Value Function vs Inventory Level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0496451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot policies\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "x_vals = range(K + 1)\n",
    "ax.plot(x_vals, policy_vfi, 'o-', linewidth=2, markersize=6,\n",
    "        label='VFI Policy')\n",
    "ax.plot(x_vals, policy_hpi, '--s', linewidth=2, markersize=4,\n",
    "        label='HPI Policy')\n",
    "ax.set_xlabel('inventory level')\n",
    "ax.set_ylabel('order amount')\n",
    "ax.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a00dd52",
   "metadata": {},
   "source": [
    "Notice that\n",
    "\n",
    "- Both VFI and HPI converge to identical solutions\n",
    "- No ordering occurs above a certain threshold\n",
    "- Orders are relatively large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfdf904",
   "metadata": {},
   "source": [
    "## Simulation of Inventory Dynamics\n",
    "\n",
    "To and understand the dynamics of the optimal policy, we simulate the inventory\n",
    "system over time using the computed optimal policy. \n",
    "\n",
    "In the code below we \n",
    "\n",
    "- Set the initial condition as: Start with maximum inventory (K units)\n",
    "- Sample from the geometric distribution using inverse CDF method\n",
    "- Use the optimal policy to determine orders at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of inventory dynamics under optimal policy\n",
    "\n",
    "# Simulation parameters\n",
    "T = 100  # number of periods\n",
    "x_start = K  # start with full inventory\n",
    "\n",
    "# Set up simulation\n",
    "key = jax.random.PRNGKey(42)\n",
    "inventory = jnp.zeros(T + 1)\n",
    "orders = jnp.zeros(T)\n",
    "demands = jnp.zeros(T)\n",
    "\n",
    "# Initial inventory\n",
    "inventory = inventory.at[0].set(x_start)\n",
    "\n",
    "# Simulate dynamics\n",
    "for t in range(T):\n",
    "    x_t = int(inventory[t])\n",
    "\n",
    "    # Optimal action from policy\n",
    "    a_t = policy_vfi[x_t]\n",
    "    orders = orders.at[t].set(a_t)\n",
    "\n",
    "    # Sample demand from geometric distribution\n",
    "    key, subkey = jax.random.split(key)\n",
    "    # Sample from geometric by inverse CDF method\n",
    "    u = jax.random.uniform(subkey)\n",
    "    demand_cdf_cumsum = jnp.cumsum(demand_pmf)\n",
    "    d_t = jnp.sum(demand_cdf_cumsum < u)\n",
    "    demands = demands.at[t].set(d_t)\n",
    "\n",
    "    # Update inventory: X_{t+1} = max(X_t - D_{t+1}, 0) + A_t\n",
    "    x_next = max(x_t - d_t, 0) + a_t\n",
    "    inventory = inventory.at[t + 1].set(x_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfc42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot simulation results\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 6))\n",
    "\n",
    "# Get matplotlib's default color cycle\n",
    "prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "colors = prop_cycle.by_key()['color']\n",
    "\n",
    "# Inventory trajectory\n",
    "ax1.plot(range(T + 1), inventory, linewidth=2, label='Inventory Level',\n",
    "         color=colors[0])\n",
    "ax1.axhline(y=0, linestyle='--', alpha=0.5, label='Stockout',\n",
    "            color=colors[1])\n",
    "ax1.axhline(y=K, linestyle='--', alpha=0.5, label='Capacity',\n",
    "            color=colors[2])\n",
    "ax1.set_ylabel('Inventory Level')\n",
    "ax1.set_title('Inventory Dynamics Under Optimal Policy')\n",
    "ax1.legend(frameon=False)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Orders\n",
    "ax2.bar(range(T), orders, alpha=0.7, label='Orders', color=colors[3])\n",
    "ax2.set_ylabel('Order Quantity')\n",
    "ax2.set_title('Ordering Decisions')\n",
    "ax2.legend(frameon=False)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Demand\n",
    "ax3.plot(range(T), demands, linewidth=2, marker='o', markersize=3,\n",
    "         label='Demand', color=colors[4])\n",
    "ax3.set_ylabel('Demand')\n",
    "ax3.set_xlabel('Time Period')\n",
    "ax3.set_title('Demand Realizations')\n",
    "ax3.legend(frameon=False)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40554fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
